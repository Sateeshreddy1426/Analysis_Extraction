{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efedeb95-afa4-4c1f-a03b-646498ae0702",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc752965-474e-4993-a680-0387c8a1a344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Netclan20241017 extracted successfully.\n",
      "Article Netclan20241018 extracted successfully.\n",
      "Article Netclan20241019 extracted successfully.\n",
      "Article Netclan20241020 extracted successfully.\n",
      "Article Netclan20241021 extracted successfully.\n",
      "Article Netclan20241022 extracted successfully.\n",
      "Article Netclan20241023 extracted successfully.\n",
      "Article Netclan20241024 extracted successfully.\n",
      "Article Netclan20241025 extracted successfully.\n",
      "Article Netclan20241026 extracted successfully.\n",
      "Article Netclan20241027 extracted successfully.\n",
      "Article Netclan20241028 extracted successfully.\n",
      "Article Netclan20241029 extracted successfully.\n",
      "Article Netclan20241030 extracted successfully.\n",
      "Article Netclan20241031 extracted successfully.\n",
      "Article Netclan20241032 extracted successfully.\n",
      "Article Netclan20241033 extracted successfully.\n",
      "Article Netclan20241034 extracted successfully.\n",
      "Article Netclan20241035 extracted successfully.\n",
      "Article Netclan20241036 extracted successfully.\n",
      "Article Netclan20241037 extracted successfully.\n",
      "Article Netclan20241038 extracted successfully.\n",
      "Article Netclan20241039 extracted successfully.\n",
      "Article Netclan20241040 extracted successfully.\n",
      "Article Netclan20241041 extracted successfully.\n",
      "Article Netclan20241042 extracted successfully.\n",
      "Article Netclan20241043 extracted successfully.\n",
      "Article Netclan20241044 extracted successfully.\n",
      "Article Netclan20241045 extracted successfully.\n",
      "Article Netclan20241046 extracted successfully.\n",
      "Article Netclan20241047 extracted successfully.\n",
      "Article Netclan20241048 extracted successfully.\n",
      "Article Netclan20241049 extracted successfully.\n",
      "Article Netclan20241050 extracted successfully.\n",
      "Article Netclan20241051 extracted successfully.\n",
      "Article Netclan20241052 extracted successfully.\n",
      "Article Netclan20241053 extracted successfully.\n",
      "Article Netclan20241054 extracted successfully.\n",
      "Article Netclan20241055 extracted successfully.\n",
      "Article Netclan20241056 extracted successfully.\n",
      "Article Netclan20241057 extracted successfully.\n",
      "Article Netclan20241058 extracted successfully.\n",
      "Article Netclan20241059 extracted successfully.\n",
      "Article Netclan20241060 extracted successfully.\n",
      "Article Netclan20241061 extracted successfully.\n",
      "Article Netclan20241062 extracted successfully.\n",
      "Article Netclan20241063 extracted successfully.\n",
      "Article Netclan20241064 extracted successfully.\n",
      "Article Netclan20241065 extracted successfully.\n",
      "Article Netclan20241066 extracted successfully.\n",
      "Article Netclan20241067 extracted successfully.\n",
      "Article Netclan20241068 extracted successfully.\n",
      "Article Netclan20241069 extracted successfully.\n",
      "Article Netclan20241070 extracted successfully.\n",
      "Article Netclan20241071 extracted successfully.\n",
      "Article Netclan20241072 extracted successfully.\n",
      "Article Netclan20241073 extracted successfully.\n",
      "Article Netclan20241074 extracted successfully.\n",
      "Article Netclan20241075 extracted successfully.\n",
      "Article Netclan20241076 extracted successfully.\n",
      "Article Netclan20241077 extracted successfully.\n",
      "Article Netclan20241078 extracted successfully.\n",
      "Article Netclan20241079 extracted successfully.\n",
      "Article Netclan20241080 extracted successfully.\n",
      "Article Netclan20241081 extracted successfully.\n",
      "Article Netclan20241082 extracted successfully.\n",
      "Article Netclan20241083 extracted successfully.\n",
      "Article Netclan20241084 extracted successfully.\n",
      "Article Netclan20241085 extracted successfully.\n",
      "Article Netclan20241086 extracted successfully.\n",
      "Article Netclan20241087 extracted successfully.\n",
      "Article Netclan20241088 extracted successfully.\n",
      "Article Netclan20241089 extracted successfully.\n",
      "Article Netclan20241090 extracted successfully.\n",
      "Article Netclan20241091 extracted successfully.\n",
      "Article Netclan20241092 extracted successfully.\n",
      "Article Netclan20241093 extracted successfully.\n",
      "Article Netclan20241094 extracted successfully.\n",
      "Article Netclan20241095 extracted successfully.\n",
      "Article Netclan20241096 extracted successfully.\n",
      "Article Netclan20241097 extracted successfully.\n",
      "Article Netclan20241098 extracted successfully.\n",
      "Article Netclan20241099 extracted successfully.\n",
      "Article Netclan20241100 extracted successfully.\n",
      "Article Netclan20241101 extracted successfully.\n",
      "Article Netclan20241102 extracted successfully.\n",
      "Article Netclan20241103 extracted successfully.\n",
      "Article Netclan20241104 extracted successfully.\n",
      "Article Netclan20241105 extracted successfully.\n",
      "Article Netclan20241106 extracted successfully.\n",
      "Article Netclan20241107 extracted successfully.\n",
      "Article Netclan20241108 extracted successfully.\n",
      "Article Netclan20241109 extracted successfully.\n",
      "Article Netclan20241110 extracted successfully.\n",
      "Article Netclan20241111 extracted successfully.\n",
      "Article Netclan20241112 extracted successfully.\n",
      "Article Netclan20241113 extracted successfully.\n",
      "Article Netclan20241114 extracted successfully.\n",
      "Article Netclan20241115 extracted successfully.\n",
      "Article Netclan20241116 extracted successfully.\n",
      "Article Netclan20241117 extracted successfully.\n",
      "Article Netclan20241118 extracted successfully.\n",
      "Article Netclan20241119 extracted successfully.\n",
      "Article Netclan20241120 extracted successfully.\n",
      "Article Netclan20241121 extracted successfully.\n",
      "Article Netclan20241122 extracted successfully.\n",
      "Article Netclan20241123 extracted successfully.\n",
      "Article Netclan20241124 extracted successfully.\n",
      "Article Netclan20241125 extracted successfully.\n",
      "Article Netclan20241126 extracted successfully.\n",
      "Article Netclan20241127 extracted successfully.\n",
      "Article Netclan20241128 extracted successfully.\n",
      "Article Netclan20241129 extracted successfully.\n",
      "Article Netclan20241130 extracted successfully.\n",
      "Article Netclan20241131 extracted successfully.\n",
      "Article Netclan20241132 extracted successfully.\n",
      "Article Netclan20241133 extracted successfully.\n",
      "Article Netclan20241134 extracted successfully.\n",
      "Article Netclan20241135 extracted successfully.\n",
      "Article Netclan20241136 extracted successfully.\n",
      "Article Netclan20241137 extracted successfully.\n",
      "Article Netclan20241138 extracted successfully.\n",
      "Article Netclan20241139 extracted successfully.\n",
      "Article Netclan20241140 extracted successfully.\n",
      "Article Netclan20241141 extracted successfully.\n",
      "Article Netclan20241142 extracted successfully.\n",
      "Article Netclan20241143 extracted successfully.\n",
      "Article Netclan20241144 extracted successfully.\n",
      "Article Netclan20241145 extracted successfully.\n",
      "Article Netclan20241146 extracted successfully.\n",
      "Article Netclan20241147 extracted successfully.\n",
      "Article Netclan20241148 extracted successfully.\n",
      "Article Netclan20241149 extracted successfully.\n",
      "Article Netclan20241150 extracted successfully.\n",
      "Article Netclan20241151 extracted successfully.\n",
      "Article Netclan20241152 extracted successfully.\n",
      "Article Netclan20241153 extracted successfully.\n",
      "Article Netclan20241154 extracted successfully.\n",
      "Article Netclan20241155 extracted successfully.\n",
      "Article Netclan20241156 extracted successfully.\n",
      "Article Netclan20241157 extracted successfully.\n",
      "Article Netclan20241158 extracted successfully.\n",
      "Article Netclan20241159 extracted successfully.\n",
      "Article Netclan20241160 extracted successfully.\n",
      "Article Netclan20241161 extracted successfully.\n",
      "Article Netclan20241162 extracted successfully.\n",
      "Article Netclan20241163 extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Loading Input Data\n",
    "input_file = 'Input.xlsx'  # Replace with your actual file name\n",
    "output_dir = 'Extracted_Articles'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Reading Input.xlsx using pandas\n",
    "data = pd.read_excel(input_file)\n",
    "\n",
    "# Function to extract article content\n",
    "def extract_article(url, url_id):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract title and text (adjust selectors based on the website structure)\n",
    "        title = soup.find('h1').get_text(strip=True)\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "        \n",
    "        # Save to text file\n",
    "        file_path = os.path.join(output_dir, f\"{url_id}.txt\")\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"{title}\\n\\n{article_text}\")\n",
    "        print(f\"Article {url_id} extracted successfully.\")\n",
    "        return title, article_text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to extract {url_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Looping through URLs and extract articles\n",
    "data['Title'] = ''\n",
    "data['Article_Text'] = ''\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    title, article_text = extract_article(url, url_id)\n",
    "    data.at[index, 'Title'] = title\n",
    "    data.at[index, 'Article_Text'] = article_text\n",
    "\n",
    "# Saving updated input file with titles and article text for reference\n",
    "data.to_excel('Updated_Input.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b08b514-90d2-4bd2-967e-e8f944b020e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\SATEESH\n",
      "[nltk_data]     REDDY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\SATEESH\n",
      "[nltk_data]     REDDY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 109\u001b[0m\n\u001b[0;32m    107\u001b[0m article_text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArticle_Text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m article_text:\n\u001b[1;32m--> 109\u001b[0m     scores \u001b[38;5;241m=\u001b[39m calculate_scores(article_text)\n\u001b[0;32m    110\u001b[0m     scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL_ID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL_ID\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    111\u001b[0m     scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[3], line 73\u001b[0m, in \u001b[0;36mcalculate_scores\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Personal Pronouns\u001b[39;00m\n\u001b[0;32m     69\u001b[0m personal_pronouns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m filtered_words \u001b[38;5;28;01mif\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb(I|we|my|ours|us)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, word, re\u001b[38;5;241m.\u001b[39mI)])\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL_ID\u001b[39m\u001b[38;5;124m'\u001b[39m:data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL_ID\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m:df1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPositive Score\u001b[39m\u001b[38;5;124m'\u001b[39m: positive_score,\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNegative Score\u001b[39m\u001b[38;5;124m'\u001b[39m: negative_score,\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolarity Score\u001b[39m\u001b[38;5;124m'\u001b[39m: polarity_score,\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubjectivity Score\u001b[39m\u001b[38;5;124m'\u001b[39m: subjectivity_score,\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg Sentence Length\u001b[39m\u001b[38;5;124m'\u001b[39m: avg_sentence_length,\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPercentage of Complex Words\u001b[39m\u001b[38;5;124m'\u001b[39m: percentage_complex_words,\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFog Index\u001b[39m\u001b[38;5;124m'\u001b[39m: fog_index,\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg Number of words per Sentence\u001b[39m\u001b[38;5;124m'\u001b[39m:avg_sentences,\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComplex Word Count\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(complex_words),\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord Count\u001b[39m\u001b[38;5;124m'\u001b[39m: word_count,\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSyllable per Word\u001b[39m\u001b[38;5;124m'\u001b[39m: syllables_per_word,\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPersonal Pronouns\u001b[39m\u001b[38;5;124m'\u001b[39m: personal_pronouns,\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAvg Word Length\u001b[39m\u001b[38;5;124m'\u001b[39m: avg_word_length\n\u001b[0;32m     87\u001b[0m     \n\u001b[0;32m     88\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df1' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Load NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load word lists\n",
    "with open('positive-words.txt', 'r') as f:\n",
    "    positive_words = set(f.read().splitlines())\n",
    "\n",
    "with open('negative-words.txt', 'r') as f:\n",
    "    negative_words = set(f.read().splitlines())\n",
    "\n",
    "with open('StopWords_Auditor.txt', 'r') as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "\n",
    "with open('StopWords_Currencies.txt', 'r') as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "\n",
    "with open('StopWords_DatesandNumbers.txt', 'r') as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "\n",
    "with open('StopWords_Generic.txt', 'r') as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "\n",
    "with open('StopWords_GenericLong.txt', 'r') as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "\n",
    "with open('StopWords_Geographic.txt', 'r') as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "\n",
    "with open('StopWords_Names.txt', 'r') as f:\n",
    "    stop_words = set(f.read().splitlines())\n",
    "    # Helper Functions\n",
    "def calculate_scores(text):\n",
    "    # Tokenize sentences and words\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text.lower())\n",
    "    filtered_words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    # Positive, Negative, and Word Count\n",
    "    positive_score = sum(1 for word in filtered_words if word in positive_words)\n",
    "    negative_score = sum(1 for word in filtered_words if word in negative_words)\n",
    "    word_count = len(filtered_words)\n",
    "\n",
    "    # Polarity and Subjectivity\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 1e-6)\n",
    "    subjectivity_score = (positive_score + negative_score) / (word_count + 1e-6)\n",
    "\n",
    "    # Average Sentence Length\n",
    "    avg_sentence_length = word_count / len(sentences)\n",
    "\n",
    "    # Complex Words and Fog Index\n",
    "    complex_words = [word for word in filtered_words if count_syllables(word) > 2]\n",
    "    percentage_complex_words = len(complex_words) / word_count * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    if len(sentences) == 0:  # Handle edge cases where no sentences exist\n",
    "        return 0\n",
    "    avg_sentences=len(filtered_words) / len(sentences)\n",
    "\n",
    "    # Syllables and Word Length\n",
    "    syllables_per_word = sum(count_syllables(word) for word in filtered_words) / word_count\n",
    "    avg_word_length = sum(len(word) for word in filtered_words) / word_count\n",
    "\n",
    "    # Personal Pronouns\n",
    "    personal_pronouns = len([word for word in filtered_words if re.match(r'\\b(I|we|my|ours|us)\\b', word, re.I)])\n",
    "\n",
    "    return {\n",
    "        'URL_ID':data['URL_ID'],\n",
    "        'URL':df1['URL'],\n",
    "        'Positive Score': positive_score,\n",
    "        'Negative Score': negative_score,\n",
    "        'Polarity Score': polarity_score,\n",
    "        'Subjectivity Score': subjectivity_score,\n",
    "        'Avg Sentence Length': avg_sentence_length,\n",
    "        'Percentage of Complex Words': percentage_complex_words,\n",
    "        'Fog Index': fog_index,\n",
    "        'Avg Number of words per Sentence':avg_sentences,\n",
    "        'Complex Word Count': len(complex_words),\n",
    "        'Word Count': word_count,\n",
    "        'Syllable per Word': syllables_per_word,\n",
    "        'Personal Pronouns': personal_pronouns,\n",
    "        'Avg Word Length': avg_word_length\n",
    "        \n",
    "    }\n",
    "\n",
    "def count_syllables(word):\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    syllable_count = 0\n",
    "    if word[0] in vowels:\n",
    "        syllable_count += 1\n",
    "    for i in range(1, len(word)):\n",
    "        if word[i] in vowels and word[i - 1] not in vowels:\n",
    "            syllable_count += 1\n",
    "    if word.endswith('e'):\n",
    "        syllable_count -= 1\n",
    "    return max(1, syllable_count)\n",
    "\n",
    "# Analyze Extracted Articles\n",
    "output_data = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    article_text = row['Article_Text']\n",
    "    if article_text:\n",
    "        scores = calculate_scores(article_text)\n",
    "        scores['URL_ID'] = row['URL_ID']\n",
    "        scores['URL']=row['URL']\n",
    "        output_data.append(scores)\n",
    "\n",
    "# Save Output to Excel\n",
    "output_df = pd.DataFrame(output_data)\n",
    "# Define column order\n",
    "column_order = ['URL_ID', 'URL', 'Positive Score', 'Negative Score', 'Polarity Score',\n",
    "                'Subjectivity Score', 'Avg Sentence Length', 'Percentage of Complex Words',\n",
    "                'Fog Index','Avg Number of words per Sentence','Complex Word Count','Word Count','Syllable per Word','Personal Pronouns','Avg Word Length']\n",
    "\n",
    "# Reorder columns in the DataFrame\n",
    "output_df = output_df[column_order]\n",
    "\n",
    "output_df.to_excel('Output_Data_Structure.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390801e-e91b-47e8-ab50-9001382a9bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"Output_Data_Structure.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43596ff5-2958-4ab8-bb75-8dc46b4952cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a5b3f-ab22-4e51-bc7e-9eb7c10d59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "# Load the dataset (Input.xlsx)\n",
    "input_data = pd.read_excel('Output_Data_Structure.xlsx')\n",
    "\n",
    "# Print the column names to ensure correct column\n",
    "print(input_data.columns.tolist())\n",
    "\n",
    "# Ensure 'URL' column exists and remove extra spaces if needed\n",
    "input_data.columns = input_data.columns.str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# Check if there are any NaN values in the 'URL' column\n",
    "if input_data['URL'].isna().sum() > 0:\n",
    "    print(f\"Warning: Found {input_data['URL'].isna().sum()} missing URLs\")\n",
    "    # Optional: Fill NaN URLs with a placeholder (or handle them differently)\n",
    "    input_data['URL'] = input_data['URL'].fillna('No URL')  # You can adjust this as needed\n",
    "\n",
    "# Create a new workbook and select the active sheet\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "# Add the header row from the original dataframe to the Excel sheet\n",
    "ws.append(input_data.columns.tolist())\n",
    "\n",
    "# Define the color for hyperlinks (e.g., blue)\n",
    "hyperlink_color = \"0000FF\"  # Blue color code\n",
    "\n",
    "# Loop through each row of the DataFrame and insert the hyperlinks for the URL column\n",
    "for index, row in input_data.iterrows():\n",
    "    row_data = row.tolist()\n",
    "    \n",
    "    # Get the URL and make it clickable by applying the HYPERLINK formula\n",
    "    url = row['URL']  # Assuming 'URL' is the column with the links\n",
    "    cell = f'=HYPERLINK(\"{url}\", \"{url}\")'\n",
    "    row_data[input_data.columns.get_loc('URL')] = cell\n",
    "    \n",
    "    # Append the modified row with the clickable URL\n",
    "    ws.append(row_data)\n",
    "    \n",
    "    # Change the font color for the hyperlink (assuming the URL column is at index 1)\n",
    "    hyperlink_cell = ws.cell(row=index + 2, column=input_data.columns.get_loc('URL') + 1)\n",
    "    hyperlink_cell.font = Font(color=hyperlink_color)\n",
    "\n",
    "# Save the new workbook to an Excel file\n",
    "wb.save('Output_Data_Structure.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79084895-e26d-403c-ac09-b62bbd12c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_excel(\"Output_Data_Structure.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4615c-a919-40ce-ad1c-74389d54a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b30e1a-3a6d-4260-93de-ba763dd56162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cffe8-f828-4ff1-a7e2-9c0f6e5ed61c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
